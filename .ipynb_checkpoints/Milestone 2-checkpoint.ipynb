{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial methods given "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how to work with the quotes data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working on colab :\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tld import get_tld\n",
    "\n",
    "def get_domain(url):\n",
    "    res = get_tld(url, as_object=True)\n",
    "    return res.tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2' \n",
    "path_to_out = '/content/quotes-2020-domains.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            urls = instance['urls'] # extracting list of links\n",
    "            domains = []\n",
    "            for url in urls:\n",
    "                tld = get_domain(url)\n",
    "                domains.append(tld)\n",
    "            instance['domains'] = domains # updating the sample with domain name\n",
    "            d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the speaker_attributes parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_attributes = pd.read_parquet('speaker_attributes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliases</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>lastrevid</th>\n",
       "      <th>ethnic_group</th>\n",
       "      <th>US_congress_bio_ID</th>\n",
       "      <th>occupation</th>\n",
       "      <th>party</th>\n",
       "      <th>academic_degree</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>candidacy</th>\n",
       "      <th>type</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Washington, President Washington, G. Washingt...</td>\n",
       "      <td>[+1732-02-22T00:00:00Z]</td>\n",
       "      <td>[Q161885, Q30]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1395141751</td>\n",
       "      <td>None</td>\n",
       "      <td>W000178</td>\n",
       "      <td>[Q82955, Q189290, Q131512, Q1734662, Q294126, ...</td>\n",
       "      <td>[Q327591]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q23</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>[Q698073, Q697949]</td>\n",
       "      <td>item</td>\n",
       "      <td>[Q682443]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Douglas Noel Adams, Douglas Noël Adams, Dougl...</td>\n",
       "      <td>[+1952-03-11T00:00:00Z]</td>\n",
       "      <td>[Q145]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1395737157</td>\n",
       "      <td>[Q7994501]</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q214917, Q28389, Q6625963, Q4853732, Q1884422...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q42</td>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Paul Marie Ghislain Otlet, Paul Marie Otlet]</td>\n",
       "      <td>[+1868-08-23T00:00:00Z]</td>\n",
       "      <td>[Q31]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1380367296</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q36180, Q40348, Q182436, Q1265807, Q205375, Q...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q1868</td>\n",
       "      <td>Paul Otlet</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[George Walker Bush, Bush Jr., Dubya, GWB, Bus...</td>\n",
       "      <td>[+1946-07-06T00:00:00Z]</td>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1395142029</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q82955, Q15982858, Q18814623, Q1028181, Q1408...</td>\n",
       "      <td>[Q29468]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q207</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>[Q327959, Q464075, Q3586276, Q4450587]</td>\n",
       "      <td>item</td>\n",
       "      <td>[Q329646, Q682443, Q33203]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Velázquez, Diego Rodríguez de Silva y Velázqu...</td>\n",
       "      <td>[+1599-06-06T00:00:00Z]</td>\n",
       "      <td>[Q29]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>1391704596</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q1028181]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Q297</td>\n",
       "      <td>Diego Velázquez</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             aliases            date_of_birth  \\\n",
       "0  [Washington, President Washington, G. Washingt...  [+1732-02-22T00:00:00Z]   \n",
       "1  [Douglas Noel Adams, Douglas Noël Adams, Dougl...  [+1952-03-11T00:00:00Z]   \n",
       "2      [Paul Marie Ghislain Otlet, Paul Marie Otlet]  [+1868-08-23T00:00:00Z]   \n",
       "3  [George Walker Bush, Bush Jr., Dubya, GWB, Bus...  [+1946-07-06T00:00:00Z]   \n",
       "4  [Velázquez, Diego Rodríguez de Silva y Velázqu...  [+1599-06-06T00:00:00Z]   \n",
       "\n",
       "      nationality      gender   lastrevid ethnic_group US_congress_bio_ID  \\\n",
       "0  [Q161885, Q30]  [Q6581097]  1395141751         None            W000178   \n",
       "1          [Q145]  [Q6581097]  1395737157   [Q7994501]               None   \n",
       "2           [Q31]  [Q6581097]  1380367296         None               None   \n",
       "3           [Q30]  [Q6581097]  1395142029         None               None   \n",
       "4           [Q29]  [Q6581097]  1391704596         None               None   \n",
       "\n",
       "                                          occupation      party  \\\n",
       "0  [Q82955, Q189290, Q131512, Q1734662, Q294126, ...  [Q327591]   \n",
       "1  [Q214917, Q28389, Q6625963, Q4853732, Q1884422...       None   \n",
       "2  [Q36180, Q40348, Q182436, Q1265807, Q205375, Q...       None   \n",
       "3  [Q82955, Q15982858, Q18814623, Q1028181, Q1408...   [Q29468]   \n",
       "4                                         [Q1028181]       None   \n",
       "\n",
       "  academic_degree     id              label  \\\n",
       "0            None    Q23  George Washington   \n",
       "1            None    Q42      Douglas Adams   \n",
       "2            None  Q1868         Paul Otlet   \n",
       "3            None   Q207     George W. Bush   \n",
       "4            None   Q297    Diego Velázquez   \n",
       "\n",
       "                                candidacy  type                    religion  \n",
       "0                      [Q698073, Q697949]  item                   [Q682443]  \n",
       "1                                    None  item                        None  \n",
       "2                                    None  item                        None  \n",
       "3  [Q327959, Q464075, Q3586276, Q4450587]  item  [Q329646, Q682443, Q33203]  \n",
       "4                                    None  item                        None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_attributes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the labels descriptions for quotebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_description = pd.read_csv('wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q31</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>country in western Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q45</th>\n",
       "      <td>Portugal</td>\n",
       "      <td>country in southwestern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q75</th>\n",
       "      <td>Internet</td>\n",
       "      <td>global system of connected computer networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q148</th>\n",
       "      <td>People's Republic of China</td>\n",
       "      <td>sovereign state in East Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q155</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>country in South America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Label                                   Description\n",
       "QID                                                                           \n",
       "Q31                      Belgium                     country in western Europe\n",
       "Q45                     Portugal                country in southwestern Europe\n",
       "Q75                     Internet  global system of connected computer networks\n",
       "Q148  People's Republic of China                  sovereign state in East Asia\n",
       "Q155                      Brazil                      country in South America"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_description.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with the features\n",
    "\n",
    "For the questions we want to use, we don't need all the features. We decided to keep the following ones:\n",
    "\n",
    "\n",
    "*   **date**: To be able to work with them.\n",
    "*   **numOccurences**: To take out the duplicates.\n",
    "*   **qids**: The speakers' id, necessary to connect with the speaker_attributes.\n",
    "*   **quotation**: To be able to assess its positivity.\n",
    "*   **quoteID**: To identify them.\n",
    "*   **speaker**: To access their attributes and see if there's multiple qids for the same speaker.\n",
    "\n",
    "For the 3 remaining features, we explain why we choosed to take them out:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### phase\n",
    "\n",
    "As we see on the [description of the phase](https://github.com/epfl-dlab/Quotebank/blob/main/phases.md). All of quotations we will use (2015-2020) are from phase E as they are past June 2014. We check below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_E = 0\n",
    "count_quote = 0\n",
    "quotes = bz2.open('quotes-2020.json.bz2', 'rb')\n",
    "\n",
    "for instance in quotes:\n",
    "    quote = json.loads(instance) # loading a sample\n",
    "    phase = quote['phase'] # extracting list of links\n",
    "    if phase == 'E':\n",
    "        count_E += 1\n",
    "    count_quote+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 100.0% of quotes in phase E for 2020\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {count_E*100/count_quote}% of quotes in phase E for 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we don't have any other phase than phase 'E' in the dataset from 2020. We shouldn't have them for 2015-2019. With the test on 2020 and the description's informations, we decided that it safe to drop this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### probas\n",
    "\n",
    "We only keep the main speaker, therefore the probabilities and name of the other speakers are not useful for us. We chose to rely on the decisions made by QuoteBanks in choosing the speaker, and only keep the main(s) one(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Urls\n",
    "\n",
    "In our research, we don't specificaly look into the source of the quote, expect for the speaker. Therefore, we decided to drop the urls feature as it is irrevelant to our research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the unwanted features\n",
    "\n",
    "We drop the unwanted features as explained before. We do this by keeping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(path_to_file, path_to_out, max_instances=-1):\n",
    "    count = 0\n",
    "    with bz2.open(path_to_file, 'rb') as s_file:\n",
    "        with bz2.open(path_to_out, 'wb') as d_file:\n",
    "            for instance in s_file:\n",
    "                if count == max_instances:\n",
    "                    return\n",
    "                instance = json.loads(instance)  # loading a sample\n",
    "                series = pd.Series(instance)\n",
    "                instance = series[[\n",
    "                    'quoteID',\n",
    "                    'date',\n",
    "                    'numOccurrences',\n",
    "                    'qids',\n",
    "                    'quotation',\n",
    "                    'speaker',\n",
    "                    ]].to_dict()\n",
    "                d_file.write((json.dumps(instance) + '\\n').encode('utf-8'))  # writing in the new file\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try it with the 100 first instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>qids</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-28-000082</td>\n",
       "      <td>2020-01-28 08:04:05</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ D ] espite the efforts of the partners to cr...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-16-000088</td>\n",
       "      <td>2020-01-16 12:00:13</td>\n",
       "      <td>1</td>\n",
       "      <td>[Q367796]</td>\n",
       "      <td>[ Department of Homeland Security ] was livid ...</td>\n",
       "      <td>Sue Myrick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-10-000142</td>\n",
       "      <td>2020-02-10 23:45:54</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>... He (Madhav) also disclosed that the illega...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-15-000053</td>\n",
       "      <td>2020-02-15 14:12:51</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>... [ I ] f it gets to the floor,</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-24-000168</td>\n",
       "      <td>2020-01-24 20:37:09</td>\n",
       "      <td>4</td>\n",
       "      <td>[Q20684375]</td>\n",
       "      <td>[ I met them ] when they just turned 4 and 7. ...</td>\n",
       "      <td>Meghan King Edmonds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                date  numOccurrences         qids  \\\n",
       "0  2020-01-28-000082 2020-01-28 08:04:05               1           []   \n",
       "1  2020-01-16-000088 2020-01-16 12:00:13               1    [Q367796]   \n",
       "2  2020-02-10-000142 2020-02-10 23:45:54               1           []   \n",
       "3  2020-02-15-000053 2020-02-15 14:12:51               2           []   \n",
       "4  2020-01-24-000168 2020-01-24 20:37:09               4  [Q20684375]   \n",
       "\n",
       "                                           quotation              speaker  \n",
       "0  [ D ] espite the efforts of the partners to cr...                 None  \n",
       "1  [ Department of Homeland Security ] was livid ...           Sue Myrick  \n",
       "2  ... He (Madhav) also disclosed that the illega...                 None  \n",
       "3                  ... [ I ] f it gets to the floor,                 None  \n",
       "4  [ I met them ] when they just turned 4 and 7. ...  Meghan King Edmonds  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file = 'quotes-2020.json.bz2'\n",
    "path_to_out = 'quotes-2020-dropped-features.json.bz2'\n",
    "\n",
    "drop_features(path_to_file, path_to_out, 100)\n",
    "\n",
    "dropped_features_df = pd.read_json(path_to_out, compression='bz2', lines=True)\n",
    "dropped_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case: when we need to have the speaker's information\n",
    "\n",
    "For two of our questions, see the [readMe.md](https://), we want to access the speaker's attributes. If we don't have any informations concerning them, the quote becomes unseful for our research. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800844\n"
     ]
    }
   ],
   "source": [
    "quotes = bz2.open('quotes-2020.json.bz2', 'rb')\n",
    "count_none = 0\n",
    "for instance in quotes:\n",
    "    quote = json.loads(instance)  # loading a sample\n",
    "    speaker = quote['speaker']  # extracting list of links\n",
    "    if speaker == 'None':\n",
    "        count_none += 1\n",
    "print(count_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 34.338097291059555% of None \n"
     ]
    }
   ],
   "source": [
    "print(f'We have {count_none*100/count_quote}% of None ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, on the quotebank data of 2020 we already have ~34% of missing speaker, labelled as \"None\". Therefore, we remove them from all the informations and write them in a new Json file.\n",
    "\n",
    "Why do we remove them? Because if we look at the exemple below, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, it is ~73% sure of not knowing who is the right speaker. We could take the as speaker the \n",
    "\n",
    "```\n",
    "['Prime Minister Netanyahu', '0.2445']\n",
    "```\n",
    "We have more chance of it being false as it has only ~24% of chances of being him. We prefer to take only the speakers with an high chance of being right. We do it in the below method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unknown_speaker(path_to_file, path_to_out, max_instances=-1):\n",
    "    count = 0\n",
    "    with bz2.open(path_to_file, 'rb') as s_file:\n",
    "        with bz2.open(path_to_out, 'wb') as d_file:\n",
    "            for instance in s_file:\n",
    "                if count == max_instances:\n",
    "                    return\n",
    "                instance = json.loads(instance)  # loading a sample\n",
    "                speaker = instance['speaker']\n",
    "                if speaker != 'None':\n",
    "                    d_file.write((json.dumps(instance) + '\\n').encode('utf-8'))  # writing in the new file\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check it with 100 000 instances (to fasten the test part):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'quotes-2020.json.bz2'\n",
    "path_to_out = 'quotes-2020-dropped_unknown_speaker.json.bz2'\n",
    "\n",
    "drop_unknown_speaker(path_to_file, path_to_out, 100000)\n",
    "\n",
    "dropped_unknown_speaker_df = pd.read_json(path_to_out, compression='bz2', lines=True)\n",
    "\n",
    "print(dropped_unknown_speaker_df.loc[dropped_unknown_speaker_df.speaker == 'None', 'speaker'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, we can safely see that we take out all the 'None' speaker from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNp2jrGVbyU-"
   },
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDmVROAJcOsw"
   },
   "source": [
    "## Dates\n",
    "\n",
    "As we are interested in dates, we want to understand their distributions.\n",
    "As the dates are in the format 'yyyy-mm-dd hh:mm:ss', we check that all the dates are well in 2020 by checking the first 4 characters. Doing so helps us to check if there's any date which is not correctly written or missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = bz2.open('quotes-2020.json.bz2', 'rb')\n",
    "for instance in quotes:\n",
    "    quote = json.loads(instance)  # loading a sample\n",
    "    date = instance['date']  # extracting the date\n",
    "    if date[:4] == '2020':\n",
    "        count_2020 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There is {count_2020*100/count_quote}% of date starting with 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uU6ivljdfV3k"
   },
   "source": [
    "So it is as guessed.\n",
    "\n",
    "To see the distribution of quotes per day of the year, we define a function to change the date format 'yyyy-mm-dd' into an index of a table. To do so we use the library date time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_day_of_year(date):\n",
    "    date_time_obj = datetime.strptime(date[2:], '%y-%m-%d %H:%M:%S')\n",
    "    d = date_time_obj.strftime('%-j')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_day_of_year(\"2020-12-30 12:26:24\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWHaocZ0i2yT"
   },
   "source": [
    "Now we can look at the distribution of the dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_per_day(path_to_file, max_instances=-1):\n",
    "    count_days = np.zeros(366)\n",
    "    count = 0\n",
    "    with bz2.open(path_to_file, 'rb') as s_file:\n",
    "        for instance in s_file:\n",
    "            if count == max_instances:\n",
    "                break\n",
    "            instance = json.loads(instance)  # loading a sample\n",
    "            date = instance['date']  # extracting the date\n",
    "            idx = int(get_day_of_year(date)) - 1\n",
    "            count_days[idx] += 1\n",
    "            count += 1\n",
    "        return count_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for the different years with a sample of 100000 quotes to have a proper representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2015','2016','2017','2018','2019','2020']\n",
    "day = np.arange(366)\n",
    "\n",
    "(fig, axes) = plt.subplots(6, constrained_layout=True, figsize=(15, 15))\n",
    "fig.suptitle('Count of quotes per day of the year', fontsize=16)\n",
    "\n",
    "for (year, ax) in zip(years, axes):\n",
    "    path_to_file = 'quotes-{year}.json.bz2'.format(year=year)\n",
    "    count_days = count_per_day(path_to_file, max_instances=100000)\n",
    "    ax.plot(day, count_days)\n",
    "    ax.set_title(year)\n",
    "    ax.set_xlabel('day of the year')\n",
    "    ax.set_ylabel('count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeAQxiCZt5e8"
   },
   "source": [
    "Everytime we look at the dates, we have to be careful about the number of quotations per day. It could lead to wrong results if we want to look at the distribution of a type of quote over time. We see an evident weekly tendency and the quotes for the year 2020 stop at ~110 days. There's also some seemingly unexplicable drop in quotes.For example, the quotes from year 2016 do some strange time where the data is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakersID = set()\n",
    "quotes = bz2.open('quotes-2020.json.bz2', 'rb')\n",
    "for q in quotes:\n",
    "    quote = json.loads(q)\n",
    "    speakers = quote['qids']\n",
    "    for speakerID in speakers:\n",
    "        speakersID.add(speakerID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323074"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speakersID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data to pair speakers and their occupations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create an array of all the different IDs for occupation and profession and removing duplicates and None values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = set()\n",
    "for w in speaker_attributes['occupation'].dropna():\n",
    "    for i in w:\n",
    "        work.add(i)\n",
    "work = list(work)\n",
    "len(work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a dictionnary which will have the QID as keys and the name and description of the job as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwork = {}\n",
    "for i in work:\n",
    "    if i in label_description.index:\n",
    "        dwork[i]=[label_description.loc[i]['Label'],label_description.loc[i]['Description']]\n",
    "len(dwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some QID had nan values we decided to count them to see how many there were and then we removed them as we couldn't use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2460\n"
     ]
    }
   ],
   "source": [
    "summ=0\n",
    "for i in dwork.values():\n",
    "    if isinstance(i[0], float):\n",
    "        summ+=1\n",
    "        \n",
    "print(\"There are\",summ,\"nan values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12098"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_work={}\n",
    "for i in dwork:\n",
    "    if not isinstance(dwork[i], float):\n",
    "        dic_work[i]=dwork[i]\n",
    "\n",
    "len(dic_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_work['Q162555']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new dataset that will help us regroup speaker regarding to their jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = pd.DataFrame(dic_work.items(), columns=['ID', 'Label'])\n",
    "df_work[['Label','Description']] = pd.DataFrame(df_work.Label.tolist(), index= df_work.index)\n",
    "df_work.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the new dataset in a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work.to_parquet('data/Id_Work.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q181217</td>\n",
       "      <td>Bey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q11087077</td>\n",
       "      <td>Periodistas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q10853156</td>\n",
       "      <td>alarife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q678003</td>\n",
       "      <td>driving instructor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q13141064</td>\n",
       "      <td>badminton player</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID               Label\n",
       "0    Q181217                 Bey\n",
       "1  Q11087077         Periodistas\n",
       "2  Q10853156             alarife\n",
       "3    Q678003  driving instructor\n",
       "4  Q13141064    badminton player"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('data/Id_Work.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to regroup the different occupations that are similar together, for example all the job realted to finance together, the one related to military together, medecine, law, etc...\n",
    "As there are thousands of different occupations it can't reasonably be done manually so we are trying to group them using NLP and clustering. We don't have a proper model yet but we can already see that with Kmeans we already got clusters that make sens. There still are some problem like one cluster having too many unrelated value, which can be fixed by increasing the number of cluster as it acts like a \"trash\" for all the value that couldn't be clustered with others. But we also see that in the same cluster we would have for example dentist and oral surgeon but then also oral story telling, so we see that there still are flaws in our model. We will keep experimenting in order to get a realy good clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.Description # Extract text\n",
    "target = df.Label # Extract target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just using the description we added the name of the occupation with it so that job with similar name would have a higher chance to be together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_desc = target.str.cat(texts, sep=' , ')\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(job_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clusters = 200\n",
    "\n",
    "model = KMeans(n_clusters=number_of_clusters, \n",
    "               init='k-means++', \n",
    "               max_iter=100, # Maximum number of iterations of the k-means algorithm for a single run.\n",
    "               n_init=1)  # Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = model.labels_\n",
    "df_cluster = df.copy()\n",
    "df_cluster['cluster'] = lab\n",
    "df_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster[df_cluster['cluster']==5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_clusters):\n",
    "    print(\"cluster\",i)\n",
    "    print(len(df_cluster[df_cluster['cluster']==i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First approach of the sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity scores of quotes using Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.79, 'pos': 0.21, 'compound': 0.872}\n",
      "{'neg': 0.167, 'neu': 0.619, 'pos': 0.214, 'compound': 0.0}\n",
      "{'neg': 0.113, 'neu': 0.84, 'pos': 0.046, 'compound': -0.4939}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.899, 'pos': 0.101, 'compound': 0.3041}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.038, 'neu': 0.888, 'pos': 0.074, 'compound': 0.3818}\n",
      "{'neg': 0.121, 'neu': 0.715, 'pos': 0.165, 'compound': 0.2484}\n",
      "{'neg': 0.0, 'neu': 0.691, 'pos': 0.309, 'compound': 0.8277}\n"
     ]
    }
   ],
   "source": [
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "path_to_file = 'quotes-2020.json.bz2' \n",
    "path_to_out = 'quotes-2020-polarity-scores.json.bz2'\n",
    "\n",
    "i = 0\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance)\n",
    "            quote = instance['quotation']\n",
    "            scores = sid.polarity_scores(quote)\n",
    "            #instance['polarity_scores'] = scores\n",
    "            print(scores)\n",
    "            i+=1\n",
    "            if(i==10):\n",
    "                break\n",
    "            # d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
